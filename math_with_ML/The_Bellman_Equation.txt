# The Bellman Equation
see : https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe07311fd5
https://blog.naver.com/PostView.naver?blogId=kryj9625&logNo=221769935372&categoryNo=15&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

V(s) = max  sigma  p(s'|s, a)[r+gamma * V(s')]
        a     s'
It requires that we update the value of the current state we are looking at in the loop
This value is calculated by considering all available actions from theat sepcific state
When we take each of those possible actions it will present us with a set of possible next states s' and respective rewards r.
So, for each of those next states s' and respecive rewards r, we performs p(s', r|s, a)[r+gamma * V(s')].

1. p(s', r|s, a) "the probability" of being in state s, taking action a, and ending up in next state s'(s' is just our transition function)
2. [r+gamma * V(s')] the reward r of ending up in next state s'(we get that from out reward function) + our discount gamma * by the value of that next state s'(we get that from our value table)
3. multiply these two parts --> p(s', r|s, a)[r+gamma * V(s')]

This calculation is just for one next state s'. We need to repeat this for each possible next state s' after taking a.
Once we have done this, we sum all the results we just got sigma s', rP(s', r|s, a) * [r + gamma * V(s')]. We then repeat this for each action a

Once these steps are complete, we will have a value associated with each possible action a from the current state we are looking at in the inner loop s.
We choose the highest using max_a and set this equal to our new value for that state V(s) <- max_a sigma s', rP(s', r|s, a) * [r + gamma * V(s')]

# sudo algorithm
Initialize V(s) arbitrarily, for all s in S
Initialize septh to a small positive value

Loop:
    delth <- 0
    Loop for each s in S:
       v <- V(s)
       v(s) <- max_a sigma s', rP(s', r|s, a) * [r + gamma * V(s')]
       delta <- max(delta, |v-V(s)|)
    until delth < septh

##############################################################################################################
# Define the transition probabilities, rewards, and discount factor for a simple gridworld
transition_probs = np.array([
    [[0.8, 0.2], [1.0, 0.0]],  # Transition probabilities for going left in state 0 and going right in state 0
    [[0.0, 1.0], [0.2, 0.8]]   # Transition probabilities for going left in state 1 and going right in state 1
])
rewards = np.array([[-1, 1], [1, -1]])
discount_factor = 0.9


# Initialize the value function
num_states = len(transition_probs)
value_function = np.zeros(num_states)


# Perform value iteration
num_iterations = 100
for _ in range(num_iterations):
    new_value_function = np.zeros(num_states)
    for state in range(num_states):
        q_values = np.zeros(2)  # Two possible actions: 0 for left, 1 for right
        for action in range(2):
            for next_state in range(num_states):
                # Calculate the Q-value for the current state-action pair
                q_values[action] += transition_probs[state, action, next_state] * (rewards[state, action] + discount_factor * value_function[next_state])
        # Update the value function with the maximum Q-value
        new_value_function[state] = np.max(q_values)
    value_function = new_value_function


# Print the resulting value function
print("Value Function:")
print(value_function)
