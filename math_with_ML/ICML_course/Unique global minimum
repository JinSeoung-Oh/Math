In the over-parametrized regime, which global minimum gets selected?

Jaim Cooper (2018):
- Generically, the set of global minimizers of the empirical risk forms a submanifold of
  dimension m âˆ’ n.

Idea of proof: Use Sardâ€™s theorem.
There are n conditions: f(x_j, Î¸) = f*(x_j), j âˆˆ [n] and Î¸ âˆˆ R^m

## The escape phenomenon
This suggests that GD(gradient decendent) solutions can be dynamically unstable for SGD.

## Quantifying this phenomenon: Linear stability
*Linearizing GD: 
 Let H(Î¸) = âˆ‡^2 RË†_n(Î¸). Linearzing GD around Î¸* gives
 Î¸_(t+1) âˆ’ Î¸* = Î¸_t âˆ’ Î¸* âˆ’ Î·(âˆ‡RË†_n (Î¸) âˆ’ âˆ‡RË†_n(Î¸*))
 â‰ˆ (I âˆ’ Î· H(Î¸*))^t(Î¸_0 âˆ’ Î¸*).

*Stability condition:
 Î»_1(H(Î¸*)) â‰¤ (2/Î·)
  flatness

## The edge of stability (EoS)
In practice, GD often settles at the edge of stability (EoS) 
###################################################################################
#     Î·               0.01          0.05        0.1           0.5           1     #
#FashionMNIST     53.5 Â± 4.3    39.3 Â± 0.5   19.6 Â± 0.15   3.9 Â± 0.0    1.9 Â± 0.0 #
#  CIFAR10        198.9 Â± 0.6   39.8 Â± 0.2   19.8 Â± 0.1    3.6 Â± 0.4        -     #
#prediction 2/Î·       200           40           20            4            2     #
###################################################################################

## Linear stability analysis for SGD
* Over-parametrized: Assume RË†_n(Î¸*) = 0 for the global minimum Î¸*
* Linearizing the SGD dynamics around Î¸* :
  ËœÎ¸_(t+1) = ËœÎ¸_t âˆ’(Î·/B)[sigma jâˆˆI_t]âˆ‡^2â„“_j(Î¸*)(ËœÎ¸_t âˆ’ Î¸*).
  and let H-j = âˆ‡^2 â„“_j(Î¸*)

## The one-dimensional case
* The SGD iteration:
  Î¸_(t+1) = (1 âˆ’ Î· 1/B [sigma jâˆˆI_t] H_j)Î¸_t,
  EÎ¸_(t+1) = (1 âˆ’ Î·a)EÎ¸_t,
  EÎ¸^2_(t+1) =[(1 âˆ’ Î·a)^2 + (Î·^2/B)s^2]EÎ¸^2_t,
where
  a =1/n [sigma i=1 to i=n] H_i, â€œsharpnessâ€
  s =np.root((1/n)[sigma i=1 to i=n] H^2_i - H^2), â€œnon-uniformityâ€
*Stability condition:
  (1 âˆ’ Î·a)^2 + (Î·^2/B)s^2 â‰¤ 1.

## The stability digram
* The learning rate and batch size play different roles in the global minima selection.
* Compared with GD, SGD prefers more uniform solutions

## Extension to high dimensions
* Similar analyses can be extended for high-dimensional cases
  Î»_max {(I âˆ’ Î·H)^2 + (Î·^2/B)Î£}â‰¤ 1,
where,
  H =(1/n) [sigma i]H_i, Î£ = (1/n)H^2_i âˆ’ H^2

* Simplification: Let
  a = Î»_max(H), s^2 = Î»_max(Î£)
then a necessary condition is
  0 â‰¤ a â‰¤ (2/Î·), 0 â‰¤ s â‰¤ (âˆšB/Î·).

## Flat minima hypothesis
SGD converges to flatter solutions and flatter solutions generalize better.

## Exploring the global minima manifold
* For over-parameterized models, global minima form a submanifold
* SGD oscillates around the manifold, bouncing back and forth inside the valley
* Claim: it moves slowly towards flatter minimum on the manifold

## Effective dynamics close to the minima manifold
* Consider the SDE approximating SGD
  d_(x_t) = âˆ’âˆ‡f(x_t)dt +âˆšÎ· D(x_t)dW_t

* For a simple example, let f(x, y) = h(x)y^2, h(x) > 0. The global minima manifold is
given by: {y = 0}.
Assume the noise covariance is proportional to the Hessian on the minima manifold:
 D^2(x) = (Ïƒ^2/2)âˆ‡^2f(x, 0) = |0    0    |
                               |0  Ïƒ^2h(x)|
* The SDE(The original dynamics) can be written as
   d_(x_t) = âˆ’h'(x_t)y^2_t dt
   d_(y_t) =  2h(x_t)y_t dt + np.root(Î·h(x_t)Ïƒ dW_t).

Close to the minima manifold, y_t is small. Hence, the x-dynamics is much slower than the
y-dynamics.
* Quasi-static analysis: Assumes y_t is close to the equilibrium given x_t:
   dx_t = âˆ’E_y h'(x_t)y^2_(t,âˆž)dt
   dy_(t,Ï„) = 2h(x_t)y_(t,Ï„)d_Ï„ + np.root(Î·h(x_t)ÏƒdW_Ï„
* The local equilibrium for y is given by y_(t,âˆž) âˆ¼ N (0,(Î·Ïƒ^2/4))). Hence we have
  dx_t/dt = âˆ’(Î·Ïƒ^2h'(x_t)/4)
* This is a gradient flow that minimizes h (the flatness)!

----------------------------------------------------------------------------------------------------------------
## Unsupervised learning: Approximating probability distributions
The memorization phenomenon: The training process ultimately converges to the
empirical distribution P_*^(n)

Can early stopping give us approximations whose error rate does not suffer
from CoD?

## The Curse of Memory in Approximation by RNNs
Theorem
Let {H^*_t}_(tâˆˆR) be a family of continuous, linear, causal, regular and time-homogeneous target functionals
Suppose there exist constants Î± âˆˆ N_+, Î², Î³ > 0 such that
y_i(t) := H*_t(e_i) âˆˆ C^(Î±+1) (R), where e_i(s) := e_i 1{sâ‰¥0} with {e_i}^d_i=1 as standard basis vectors in R^d, 
and e^(Î²t)y^(k)_i(t) = o(1) as t â†’ +âˆž, with sup_(tâ‰¥0) (|e^Î²t y^(k)_i(t)|)/Î²^k â‰¤ Î³, i = 1, . . . , d,
k = 1, . . . , Î± + 1.  Then there exists a universal constant C(Î±) > 0 only depending on Î±,
such that for any m âˆˆ N+, there exists a sequence of width-m RNN functionals s {HË†_t}_(tâˆˆR)
such that
  sup ||H*_t - H^_t|| <= (C(Î±)Î³d)/Î²m^Î±
  tâˆˆR

Curse of memory: H*_t(e1) âˆ¼ t^(âˆ’Ï‰) â‡’ m âˆ¼ O(Ï‰^âˆ’(1/Ï‰)).

## Reinforcement learning
Existing work focuses on the â€œclassicalâ€ situation when the state and action spaces are finite
(and small).
Almost no work for the situation when the state/action spaces are big or high dimensional, in
which case we must use function approximation.

Difficulty: Reinforcement learning involves all the aspects discussed so far:
* function approximation
* learning dynamical systems
* learning probability distributions
* generalization gap
* training is done online and can not be decouple

## The central theme is about understanding high dimensional functions
## A reasonable mathematical picture is taking shape.
   * approximation theory in high dimension
   * global minimum selection and late stage training
## Theorems vs. insight
   * carefully designed numerical experiments
   * asymptotic analysis
